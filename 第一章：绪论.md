# 第一章：绪论

彭正皓

[TOC]

1.1 Reinforcement Learning

1.2 Examples

1.3 Elements of Reinforcement Learning

1.4 Limitations and Scope

1.5 An Extended Example: Tic-Tac-Toe

1.6 Summary

## 1.7 Early History of Reinforcement Learning

### Trial&Error流派

本流派

* 从动物心理学研究出发
* 经历了最早期的AI研究
* 带来了八十年代RL复兴
* 动物回倾向于，在同一个场景下，重复那些曾使它获得快乐的动作，反之亦然。（Law of Effect）
* Pavlov定义reinforcer：强化或弱化动物的行为的事物，在其移除之后其作用仍然生效。
* 人们发明了一些解迷宫的机器，在五十年代。
* 从那时起，有人就搞混了RL和有监督学习的关系。他们最大的不同在于，在RL中，选取一个Action不依赖于了解啥才是“correct action”。
* Minsky，1961的一篇文章《Steps Toward Artificial Intelligence》探讨了trial&error方法的许多问题，如prediction、expectation和credit-assignment问题。（作者认为这文很值得一读）
* 在六七十年代，很少文章着眼与Trial and Error流派的计算方面。



### 最优控制流派

本流派

* 关注最优控制，和其基于动态规划、价值函数的Solution。
* 基本不用learning

**最优控制**：

* 五十年代开始
* 设计一个控制器以最大化或最小化一个动态系统的某种指标

#### 1. Dynamic Programming

时间：mid-1950s

人物：Richard Bellman

* 利用了dynamical systems's state和optimal return function (value function)的概念
* 定义了Bellman方程
* 解Bellman方程就可以解最优控制问题
* Bellman引入了离散版本的最优控制问题，即Markov Decision Processes （MDPs）（换言之最优控制问题本身是连续的）
* Policy Iteration是MDPs的一个方法。
* 动态规划被认为是解General Stochastic Optimal Control Problems的唯一方法。
* 但是遭受“维度诅咒”。

#### 2.  动态规划和学习的关系

* 动态规划、最优控制、学习，这三者的关联：
  * 动态规划依赖于精确的系统模型和Bellman方程的解析解。
  * 动态规划有一个反向的计算。而学习有一个前向的计算。
* “Approximate Dynamic Programming”，即融合RL和DP的诸多方法，都试图在弥合DP的一些传统缺陷。

#### 3. 动态规划、最优控制，皆是RL

* 尽管一些传统方法需要对系统的所有知识，但是最优控制，尤其是被定义成MDPs这种形式的最优控制问题真的很像RL。
* 有一些DP解法需要不断迭代，逐渐收敛于最优解。这很像learning。



### Temporal Difference流派

结合了前面两个流派。

* 着眼于对于一个量的时序上的连续的估计的difference。
* TD方法仿佛是与RL完全不同的方法。
* TD方法最开始来自于动物心理学实验。
* Klopf在1970s复兴了Trial and Error流派。同时也提到了TD流派。
* 后来Sutton将TD和Trial and Error联合起来，得到了actor-critic框架。
* 1988年Sutton将TD learning分离出来，作为一种独立的Prediction Method，即$TD(\lambda)$算法，并证明了其收敛性。
* 1989年Watkins的Q-learning将最优控制和TD联合起来。